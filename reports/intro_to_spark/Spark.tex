\documentclass[9pt,twocolumn,twoside]{idsi}
% Defines a new command for the horizontal lines, change thickness here
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\renewcommand{\headrulewidth}{2pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
%        \includegraphics[scale=0.15]{figs/ncsa_vertical} 
    \end{tabular}
  }
  \fancyhead[C]{
      	\begin{tabular}[m]{c}
		  	\fontsize{20}{20} Illinois Data Science Initiative    	
		\end{tabular}
  }

  \fancyhead[R]{
    \begin{tabular}{ll}
%	  	\includegraphics[scale=0.125]{figs/ill}  		
  	\end{tabular}
  }
  
  \fancyfoot[C]{\thepage}
}
\pagestyle{plain}
\def \report_title {Setting up Spark for Python}
\author[1]{Sameet Sapra}
\author[2]{Joshua Chang}
\author[3]{Professor Brunner}
\title{Setting up Spark for Python}

\begin{abstract}
An introduction to setting up Apache Spark with Python
\end{abstract}

\usepackage{listings}
\begin{document}

\begin{titlepage}
\center 
\textsc{\LARGE Illinois Data Science Initiative}\\[1.5cm] 
\textsc{\Large Technical Reports}\\[0.5cm] \HRule \\[0.4cm]
{\huge \bfseries Setting up Spark for Python } \\[0.4cm] \HRule \\[1.5cm]
\Large \emph{Author:}\\ Sameet Sapra \& Joshua Chang \& Professor Brunner\\[3cm]
{\large \today}\\[3cm] % Date
%\includegraphics{Logo}\\[1cm] % uncomment if you want to place a logo
\vfill
\end{titlepage}
%\include{cover}

\maketitle

\section{What is Apache Spark?}

Apache Spark is a fast cluster computing system. Spark can be configured with multiple cluster managers like Yarn, Mesos, Amazon EC2, or standalone mode. This technical report will install Spark on top of Yarn because Hadoop MapReduce is a feature that ships with Yarn.

Spark supports many high level tools like GraphX and MLlib, for graphs processing and machine learning. It also has many APIs in Java, Scala, and Python, and we will install and configure the Java APIs and PySpark, Python's API for Spark.

\section{Why PySpark?}

PySpark is an API that interfaces with RDD's in Python. It is built on top of Spark's Java API and exposes the Spark programming model to Python. PySpark makes use of a library called Py4J, which enables Python programs to dynamically access Java objects in a Java Virtual Machine. This allows data to be processed in Python and cached in the JVM.

\section{PySpark Prerequisites}

Assume that the environment is CentOS. While many Spark tutorials will cover installation of Java, Scala, Python, or any other API languages, we can verify that Java and Yarn are installed by checking the versions. If they are installed, skip to the Spark configuration steps, otherwise follow the installation instructions given below.

\section{Installation: Hadoop with Yarn}
\noindent
You can install Java on the system with:
\begin{verbatim}
> sudo yum install java-1.7.0-openjdk-devel
\end{verbatim}

\noindent
Next we will install Hadoop, since Yarn ships with it. To install Hadoop, we will create a Hadoop user account, install Hadoop, and set environment variables:

\begin{lstlisting}[breaklines]
> useradd -d /opt/hadoop hadoop
> passwd hadoop
> curl -O http://apache.javapipe.com/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz 
> tar xfz hadoop-2.7.2.tar.gz
> cp -rf hadoop-2.7.2/* /opt/hadoop/
> chown -R hadoop:hadoop /opt/hadoop/
> su - hadoop
\end{lstlisting}

Open up the $.bashrc$ file and set the following environment variables:
\begin{lstlisting}[breaklines]
> vi .bashrc

export HDP=/usr/hdp/current/hadoop-mapreduce-client
export STREAMING=/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar
export EXAMPLES=/usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
export MAP=yarn
\end{lstlisting}

Exit out and reload the bashrc with:
\begin{verbatim}
> source $HOME/.bash_profile
\end{verbatim}

To test that it is running correctly, try creating a random directory in the HDFS file system:
\begin{verbatim}
> hdfs dfs -mkdir /my_storage
\end{verbatim}

Exit out of the hadoop user role. Now we will move onto installing Spark.

\section{Installation: Spark}
\noindent
Install Spark on the system by downloading the rpm and moving it to the correct location in your path environment variable. Here we will install spark-1.6:
\begin{lstlisting}[breaklines]
> wget http://apache.mirrors.ionfish.org/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz
> tar -zxvf spark-1.6.0-bin-hadoop2.6.tgz
> mv spark-1.6.0-bin-hadoop2.6 spark
\end{lstlisting}

\noindent
Let's configure Spark to only show errors on startup, rather than all info. This will make the Spark shell less cluttered when it is opened on startup.
\begin{verbatim}
> cd /usr/hdp/2.3.6.0-3796/spark/conf
> vi log4j.properties
\end{verbatim}

\noindent
Find a line that describe the rootCategory of log:
\begin{verbatim}
log4j.rootCategory=INFO, console
\end{verbatim}

Replace INFO with ERROR:
\begin{verbatim}
log4j.rootCategory=ERROR, console
\end{verbatim}

If all that worked, you should see the Spark shell start up when you type
\begin{verbatim}
> spark-shell
\end{verbatim}

\section{Setting up PySpark}

Now let's ensure that Python 2.7 is installed and configured.
\begin{verbatim}
> yum install -y centos-release-SCL
> yum install -y python27
> yum -y install python-pip
\end{verbatim}

\noindent
Finally, let's set up the environment variables for Spark and Python.
\begin{verbatim}
> vi $HOME/.bashrc
> export SPARK_HOME=/usr/hdp/2.3.6.0-3796/spark
> export PYTHONPATH=$SPARK_HOME/python
> export SPARK_HIVE=true
\end{verbatim}

\noindent
Then, reload the environment:
\begin{verbatim}
> source $HOME/.bashrc
\end{verbatim}

Again, let's make sure Python is setup correctly. Type
\begin{verbatim}
> python
\end{verbatim}
You should see the Python REPL come up. Finally, we can start writing code in Python.

\section{Writing our first lines of PySpark}

This example is taken from the Apache Spark website. It runs a parallelized operation to compute the value of $\pi$ and is a good example to see the benefits of Spark.

\begin{lstlisting}[language=Python]
def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)
\end{lstlisting}

Once you've written your Python code, you can compile and deploy it with:
\begin{verbatim}
> spark-submit --master yarn-cluster MY_PYTHON_FILE.py
\end{verbatim}

\noindent
The \textit{'--num-executors 10'} flag (arbitrary number) may be added to specify how many executors, the objects responsible for executing tasks, are to be used. Using as many executors as data nodes is recommended to minimize the runtime.

\section{Conclusion}

This technical report serves as a guide to set up an environment to run Spark on HDFS and write some simple Python code to take advantage of Spark using PySpark. With this setup, you can now take advantage of Spark's Resilient Distributed Datasets and have the data stored in memory and more quickly accessible.

\end{document}
