\documentclass[9pt,twocolumn,twoside]{idsi}
\usepackage{listings}
% Defines a new command for the horizontal lines, change thickness here
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\renewcommand{\headrulewidth}{2pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
       % \includegraphics[scale=0.15]{template/figs/ncsa_vertical} 
    \end{tabular}
  }
  \fancyhead[C]{
        \begin{tabular}[m]{c}
        \fontsize{20}{20} Illinois Data Science Initiative      
    \end{tabular}
  }

  \fancyhead[R]{
    \begin{tabular}{ll}
      % \includegraphics[scale=0.125]{template/figs/ill}      
    \end{tabular}
  }
  
  \fancyfoot[C]{\thepage}
}
\pagestyle{plain}
\def \reporttitle {PySpark on Python 3: Configuration and Package Management Guide}
\author[1]{Benjamin Congdon}
\author[2]{Professor Robert J. Brunner}
\affil[1]{National Center For Supercomputing Applications (NCSA)}
\affil[2]{Laboratory for Computation, Data, and Machine Learning}
\title{\reporttitle}

\begin{abstract}
PySpark requires requires Python 2.6 or later, and does not officially support Python 3. However, with only minor configuration changes, it is possible to successfully run a Spark cluster using Python 3 for both PySpark drivers and workers.
\end{abstract}

\begin{document}

\begin{titlepage}
\center 
\textsc{\LARGE Illinois Data Science Initiative}\\[1.5cm] 
\textsc{\Large Technical Reports}\\[0.5cm] \HRule \\[0.4cm]
{\huge \bfseries \reporttitle } \\[0.4cm] \HRule \\[1.5cm]
\Large \emph{Author:}\\ Benjamin Congdon\\[3cm]
{\large \today}\\[3cm] % Date
% \includegraphics{Logo}\\[1cm] % uncomment if you want to place a logo
\vfill
\end{titlepage}
\include{cover}

\maketitle

\section{Introduction}
This technical report will describe a process for upgrading a Spark Cluster to operate with Python 3 as the version of Python used on worker instances and the PySpark driver. Additionally, this report will suggest a maintainable method for installing and upgrading a set of Python packages on a Spark Cluster for use in PySpark jobs.

\section{Assumptions}
This technical report will make the following assumptions:
\begin{itemize}
  \item The implementer already has a working Spark Cluster with a CentOS worker base image.
  \item Ideally, the Spark Cluster is managed by Apache Ambari. (This report will still contain valid information for non-Ambari-managed clusters, but the configuration files will have to be manually adjusted on every node).
\end{itemize}

\section{Motivation}
While Python 3 is not yet the \emph{de facto} standard in the Python community, the trend seams to be moving in the direction of Python 3 adoption. Thus, it behooves members of the data science community to begin to transition to Python 3 for cloud computing.

Additionally, we observed that running the default Python version that shipped with our CentOS image, Python 2.6.6, had many more issues being installed concurrently with Python 2.7 than any version of Python 3. PySpark requires Python 2.7+, and Ambari requires Python 2.6.6. As a result, we found that a more stable equilibrium was met while running our PySpark workers on Python 3 and leaving the Python 2.* installation on the workers undisturbed.

Furthermore, we researched the available solutions for maintaining and distributing Python packages across our cluster, and felt that the built-in mechanisms for distributing packages at runtime (i.e. the \texttt{-\--py-files} argument for spark-submit) to be lacking. Not only does this require the operator to have built versions of all the packages they wish to include in their job, but it also means that using packages like \texttt{nltk}, which download external datasets, are difficult to include in jobs.

\section{Installing Python 3}
To run the Spark Cluster on top of Python 3, an identical version Python 3 should be installed on all data nodes, and anywhere that a driver will be run.

Usually, this install process is rather simple: use the package manager for your OS to install the current Python 3 package.

\begin{verbatim}
# Ubuntu, Debian
$ apt-get install python3

# RHEL, CentOS
$ yum install python3
\end{verbatim}

The complexity comes in the fact that you'll have to install Python 3 on all the nodes in your cluster. While this can be accomplished in a variety of ways, we suggest that you use Fabric to perform the operation. A link to the Fabfile used to install Python 3 is included in the references of this technical report.

\section{Configuring PySpark to run Python 3}

There are 2 primary configuration settings necessary to configure PySpark to work with Python 3: \texttt{PYSPARK\_PYTHON} and \texttt{PYSPARK\_DRIVER\_PYTHON}. The former dictates the path to the Python executable used for the workers, and the latter dictates the path to the Python executable used for the client driver, when applicable.

When running in a \emph{client} mode, these variables can be set as environment variables on the node being used as the client driver. However, when PySpark is used in a \emph{cluster} mode (such as \emph{yarn-cluster}), this setting must be configured in the Spark configuration.

There is one additional complication due to the way that Python 3 handles hashing. Python 3 introduces randomness into its hashes to prevent hashing DoS attacks. However, in our parallel computing, we need nodes to agree on the hash seed. Thus, we must also configure \texttt{PYTHON\_HASH\_SEED} to be identical on all nodes in the cluster.

To make the relevant changes to your Spark configuration, go to the Ambari Dashboard and navigate to \emph{Services > Spark > Configs}.

Add the following entries to \emph{spark-env template}:

\begin{lstlisting}
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
export PYTHONHASHSEED=123
\end{lstlisting}

Add the following entries to \emph{Custom spark-defaults}
\begin{itemize}
  \item spark.executorEnv.PYTHONHASHSEED $\rightarrow$ 0

  \item spark.pyspark.driver.python $\rightarrow$ /usr/bin/python3

  \item spark.pyspark.python $\rightarrow$ /usr/bin/python3

  \item spark.yarn.appMasterEnv.PYSPARK\_DRIVER\_PYTHON $\rightarrow$ /usr/bin/python3

  \item spark.yarn.appMasterEnv.PYSPARK\_PYTHON $\rightarrow$ /usr/bin/python3
\end{itemize}

Note that \texttt{/usr/bin/python3} is the default install location for most Python 3 installations (including the CentOS package). However, if you have Python 3 installed at a different location, you will have to adjust this path as necessary.

Once these changes have been made, save the configuration and restart all the affected nodes.

\section{Installing and Updating Packages}

Python maintains a store of system \emph{site-packages}. To use these packages in your PySpark jobs, each package you want to import will have to be installed on all nodes. As Python 3 will have a separate set of installed packages than Python 2, you will need to reinstall all downloaded packages following the Python 3 upgrade.

Again, we recommend using Fabric to manage Python package distributions. You can find a link to a Fabfile that installs packages to cluster nodes in the references of this technical report.

We have found it useful to maintain a \texttt{requirements.txt} file for the packages on the cluster. A \texttt{requirements.txt} file maintains a list of packages installed, and the current installed version. This is similar to \texttt{Gemfile.lock} for Ruby Gems. A typical \texttt{requirements.txt} file looks similar to this:

\begin{verbatim}
appdirs==1.4.0
beautifulsoup4==4.4.0
nose==1.3.7
packaging==16.8
pyparsing==2.1.10
requests==2.5.0
six==1.10.0
\end{verbatim}

Keeping package versions consistent across nodes is important, as is assuring all required packages are installed on all nodes. Failure to do so will lead to unpredictable behavior and the possibility of Spark job failure.

\subsection{Installing New Packages}

When a cluster user wants to make add a package to the cluster, have them add the package and desired version to the cluster's \texttt{requirements.txt}. The installed version of a package can be obtained with this command:
\begin{verbatim}
$ pip freeze | grep <Package>
\end{verbatim}

For example, we can search for the latest version of \texttt{beautifulsoup4}:
\begin{verbatim}
$ pip freeze | grep beautifulsoup4
beautifulsoup4==4.4.0
\end{verbatim}

Once the cluster's \texttt{requirements.txt} has been updated, run the package installation Fabfile task to update the \emph{site-packages} of all the nodes in the cluster.

\subsection{Updating Packages}
Updating packages can be done manually, by changing the version required in the cluster's \texttt{requirements.txt}. To update all packages automatically, you can run \texttt{pip} in a Python \texttt{virtualenv} with its update flag to pull down the latest version numbers. (Note that this requires \texttt{virtualenv} to be installed.)

\begin{verbatim}
$ virtualenv venv
$ source venv/bin/activate
$ pip install -r requirements.txt --upgrade
$ pip freeze > requirements.txt
$ deactivate
$ rm -r venv/
\end{verbatim}

Once the \texttt{requirements.txt} has been updated, run the install Fabric task again and the nodes will be updated similarly.

\section{Conclusion}  

\section*{References}
\begin{itemize}
  \item <Reference to Fabric setup technical report>
  \item <Reference to Python3 install fabfile>
  \item <Reference to Fabric requirements.txt Fabfile>
\end{itemize}


\end{document}